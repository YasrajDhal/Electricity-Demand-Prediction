{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YasrajDhal/Electricity-Demand-Prediction/blob/main/Energy%20consumption%20prediction%20using%20LSTM-GRU%20in%20PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmmC1yKJ1v9i"
      },
      "source": [
        "# Energy consumption prediction using LSTM/GRU in PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNaDyQV81v9j"
      },
      "source": [
        "In this notebook, we'll be using GRU and LSTM models for a time series prediction task and we will compare the performance of the GRU model against an LSTM model as well. The dataset that we will be using is the Hourly Energy Consumption dataset which can be found on [Kaggle](https://www.kaggle.com/robikscube/hourly-energy-consumption). The dataset contains power consumption data across different regions around the United States recorded on an hourly basis.\n",
        "\n",
        "The goal of this implementation is to **create a model that can accurately predict the energy usage in the next hour** given historical usage data. We will be using both the GRU and LSTM model to train on a set of historical data and evaluate both models on an unseen test set. To do so, we'll start with feature selection, data-preprocessing, followed by defining, training and eventually evaluating the models.\n",
        "\n",
        "We will use the PyTorch library to implement both types of models along with other common Python libraries used in data analytics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeVZ27bi1v9k"
      },
      "source": [
        "## GRU/LSTM cells"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2P6rOZYW1v9k"
      },
      "source": [
        "* Long Short-Term Memory networks (LSTMs) have great memories and can remember information which the vanilla RNN is unable to!\n",
        "\n",
        "* The Gated Recurrent Unit (GRU) is the younger sibling of the more popular Long Short-Term Memory (LSTM) network, and also a type of Recurrent Neural Network (RNN). Just like its sibling, GRUs are able to effectively retain long-term dependencies in sequential data. And additionally, they can address the “short-term memory” issue plaguing vanilla RNNs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwWkxEwx1v9l"
      },
      "source": [
        "## The ML Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_tSjwbex1v9l"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "from tqdm.notebook import tqdm as tqdm_notebook\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmtDvJjI1v9l",
        "outputId": "e94e4d92-969b-4d75-ca84-436bc19f4904"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5.1+cu124\n"
          ]
        }
      ],
      "source": [
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yp99LOQE1v9m"
      },
      "source": [
        "## Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2CyjidF1v9m",
        "outputId": "a131cbb5-e465-430f-b6f3-f4d4d63d52ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "data_dir='/content/drive/MyDrive/electricity_data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "F7E6XSAj1v9m",
        "outputId": "83d94726-98f2-475b-de62-8e087d0668fa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              Datetime  DEOK_MW\n",
              "0  2012-12-31 01:00:00   2945.0\n",
              "1  2012-12-31 02:00:00   2868.0\n",
              "2  2012-12-31 03:00:00   2812.0\n",
              "3  2012-12-31 04:00:00   2812.0\n",
              "4  2012-12-31 05:00:00   2860.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0932a53c-a2c5-4914-b3fd-0f31c1b0bb37\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Datetime</th>\n",
              "      <th>DEOK_MW</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2012-12-31 01:00:00</td>\n",
              "      <td>2945.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2012-12-31 02:00:00</td>\n",
              "      <td>2868.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2012-12-31 03:00:00</td>\n",
              "      <td>2812.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2012-12-31 04:00:00</td>\n",
              "      <td>2812.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2012-12-31 05:00:00</td>\n",
              "      <td>2860.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0932a53c-a2c5-4914-b3fd-0f31c1b0bb37')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0932a53c-a2c5-4914-b3fd-0f31c1b0bb37 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0932a53c-a2c5-4914-b3fd-0f31c1b0bb37');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-bec0861f-300d-49f3-ac97-b7a7159df786\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bec0861f-300d-49f3-ac97-b7a7159df786')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-bec0861f-300d-49f3-ac97-b7a7159df786 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Datetime\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"2012-12-31 02:00:00\",\n          \"2012-12-31 05:00:00\",\n          \"2012-12-31 03:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"DEOK_MW\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 54.53255908170824,\n        \"min\": 2812.0,\n        \"max\": 2945.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2868.0,\n          2860.0,\n          2945.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "pd.read_csv(os.path.join(data_dir, \"DEOK_hourly.csv\")).head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5TkOJTY1v9m"
      },
      "source": [
        "We have a total of **12** *.csv* files containing hourly energy trend data (*'est_hourly.paruqet'* and *'pjm_hourly_est.csv'* are not used). In our next step, we will be reading these files and pre-processing these data in this order:\n",
        "- Getting the time data of each individual time step and generalizing them\n",
        "    - Hour of the day *i.e. 0-23*\n",
        "    - Day of the week *i.e. 1-7*\n",
        "    - Month *i.e. 1-12*\n",
        "    - Day of the year *i.e. 1-365*\n",
        "    \n",
        "    \n",
        "- Scale the data to values between 0 and 1\n",
        "    - Algorithms tend to perform better or converge faster when features are on a relatively similar scale and/or close to normally distributed\n",
        "    - Scaling preserves the shape of the original distribution and doesn't reduce the importance of outliers\n",
        "    \n",
        "    \n",
        "- Group the data into sequences to be used as inputs to the model and store their corresponding labels\n",
        "    - The **sequence length** or **window_size period** is the number of data points in history that the model will use to make the prediction\n",
        "    - The label will be the next data point in time after the last one in the input sequence\n",
        "    \n",
        "\n",
        "- The inputs and labels will then be split into training and test sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keRrZn0K1v9m"
      },
      "source": [
        "## Create training instances by moving sliding window"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "sYCDr0Cf1v9m"
      },
      "outputs": [],
      "source": [
        "def move_sliding_window(data, window_size, inputs_cols_indices, label_col_index):\n",
        "    \"\"\"\n",
        "    data: numpy array including data\n",
        "    window_size: size of window\n",
        "    inputs_cols_indices: col indices to include\n",
        "    \"\"\"\n",
        "\n",
        "    # (# instances created by movement, seq_len (timestamps), # features (input_len))\n",
        "    inputs = np.zeros((len(data) - window_size, window_size, len(inputs_cols_indices)))\n",
        "    labels = np.zeros(len(data) - window_size)\n",
        "\n",
        "    for i in range(window_size, len(data)):\n",
        "        inputs[i - window_size] = data[i - window_size : i, inputs_cols_indices]\n",
        "        labels[i - window_size] = data[i, label_col_index]\n",
        "    inputs = inputs.reshape(-1, window_size, len(inputs_cols_indices))\n",
        "    labels = labels.reshape(-1, 1)\n",
        "    print(inputs.shape, labels.shape)\n",
        "\n",
        "    return inputs, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "au7epsRj1v9n"
      },
      "source": [
        "## Integrate files to build the training set\n",
        "To speed things up, I will only be using `num_files_for_dataset` .csv files for creating my dataset. Feel free to run it yourself with the entire dataset if you have the time and computing capacity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231,
          "referenced_widgets": [
            "ebe6b66656b84c7f8bb2555888b4cf0b",
            "070d0424f08c45c19d91419307d05b9d",
            "bcdf2cccc4bc4f0fa352e9393a530b74",
            "69810c5b35174d36b4051ae8bd9fe09b",
            "ccc40be9df774338811495900cd07804",
            "3bb9fba394c8453a80ced5c821a133b5",
            "8ca3a5f8262745d9aae7b540fd6a91ec",
            "eef400e09ea342dfb7e1011dc45bb4a8",
            "b73b3fc743ae42aca4c14b3b3ef0bdc6",
            "e636b940965646bfb7f4c6ff3d58df50",
            "7b7b8c041d3e4f22bc01212a0212b241"
          ]
        },
        "id": "wx53RkBE1v9n",
        "outputId": "470b4627-f222-4019-b37e-98320a3aebf2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ebe6b66656b84c7f8bb2555888b4cf0b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing AEP_hourly.csv ...\n",
            "(121183, 90, 5) (121183, 1)\n",
            "Processing FE_hourly.csv ...\n",
            "(62784, 90, 5) (62784, 1)\n",
            "Processing NI_hourly.csv ...\n",
            "(58360, 90, 5) (58360, 1)\n",
            "Processing DOM_hourly.csv ...\n",
            "(116099, 90, 5) (116099, 1)\n",
            "Processing DUQ_hourly.csv ...\n",
            "(118978, 90, 5) (118978, 1)\n"
          ]
        }
      ],
      "source": [
        "label_col_index = 0  # consumption as label to predict\n",
        "inputs_cols_indices = range(\n",
        "    5\n",
        ")  # use (consumption, hour, dayofweek, month, dayofyear) columns as features\n",
        "\n",
        "# Define window_size period and split inputs/labels\n",
        "window_size = 90\n",
        "\n",
        "# The scaler objects will be stored in this dictionary so that our output test data from the model can be re-scaled during evaluation\n",
        "label_scalers = {}\n",
        "\n",
        "train_x = []\n",
        "test_x = {}\n",
        "test_y = {}\n",
        "\n",
        "# Skipping the files we're not using\n",
        "processing_files = [\n",
        "    file for file in os.listdir(data_dir) if os.path.splitext(file)[1] == \".csv\"\n",
        "]\n",
        "\n",
        "num_files_for_dataset = 5\n",
        "\n",
        "for file in tqdm_notebook(processing_files[:num_files_for_dataset]):\n",
        "    print(f\"Processing {file} ...\")\n",
        "    # Store csv file in a Pandas DataFrame\n",
        "    df = pd.read_csv(os.path.join(data_dir, file), parse_dates=[\"Datetime\"])\n",
        "\n",
        "    # Processing the time data into suitable input formats\n",
        "    df[\"hour\"] = df.apply(lambda x: x[\"Datetime\"].hour, axis=1)\n",
        "    df[\"dayofweek\"] = df.apply(lambda x: x[\"Datetime\"].dayofweek, axis=1)\n",
        "    df[\"month\"] = df.apply(lambda x: x[\"Datetime\"].month, axis=1)\n",
        "    df[\"dayofyear\"] = df.apply(lambda x: x[\"Datetime\"].dayofyear, axis=1)\n",
        "    df = df.sort_values(\"Datetime\").drop(\"Datetime\", axis=1)\n",
        "\n",
        "    # Scaling the input data\n",
        "    sc = MinMaxScaler()\n",
        "    label_sc = MinMaxScaler()\n",
        "    data = sc.fit_transform(df.values)\n",
        "\n",
        "    # Obtaining the scaler for the labels(usage data) so that output can be\n",
        "    # re-scaled to actual value during evaluation\n",
        "    label_sc.fit(df.iloc[:, label_col_index].values.reshape(-1, 1))\n",
        "    label_scalers[file] = label_sc\n",
        "\n",
        "    # Move the window\n",
        "    inputs, labels = move_sliding_window(\n",
        "        data,\n",
        "        window_size,\n",
        "        inputs_cols_indices=inputs_cols_indices,\n",
        "        label_col_index=label_col_index,\n",
        "    )\n",
        "\n",
        "    # CONCAT created instances from all .csv files.\n",
        "    # Split data into train/test portions and combining all data from different files into a single array\n",
        "    test_portion = int(0.1 * len(inputs))\n",
        "    if len(train_x) == 0:  # first iteration\n",
        "        train_x = inputs[:-test_portion]\n",
        "        train_y = labels[:-test_portion]\n",
        "    else:\n",
        "        train_x = np.concatenate((train_x, inputs[:-test_portion]))\n",
        "        train_y = np.concatenate((train_y, labels[:-test_portion]))\n",
        "    test_x[file] = inputs[-test_portion:]\n",
        "    test_y[file] = labels[-test_portion:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tf8y4Z1u1v9n"
      },
      "source": [
        "![input_shape.png](https://github.com/iamirmasoud/energy_consumption_prediction/blob/master/imgs/input_shape.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhOzpXGF1v9n"
      },
      "source": [
        "## What have we made?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCrnqsVz1v9o",
        "outputId": "8649f901-5628-4d91-ce07-92fdc2bf9398"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((429666, 90, 5), (5836, 90, 5))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "train_x.shape, test_x[\"NI_hourly.csv\"].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJ6lx8dY1v9o"
      },
      "source": [
        "## Pytorch data loaders/generators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38WfLkn01v9o"
      },
      "source": [
        "To improve the speed of our training, we can process the data in batches so that the model does not need to update its weights as frequently. The `TensorDataset` and `DataLoader` classes are useful for splitting our data into batches and shuffling them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "04hbT0GL1v9o"
      },
      "outputs": [],
      "source": [
        "batch_size = 1024\n",
        "\n",
        "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "\n",
        "# Drop the last incomplete batch\n",
        "train_loader = DataLoader(\n",
        "    train_data, shuffle=True, batch_size=batch_size, drop_last=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcrAQQrj1v9o",
        "outputId": "be4207e5-7819-4dc4-9c6c-131d7c499ecd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Size: (429666, 90, 5), Batch Size: 1024, # of iterations per epoch: 419\n"
          ]
        }
      ],
      "source": [
        "print(\n",
        "    f\"Train Size: {train_x.shape}, Batch Size: {batch_size}, # of iterations per epoch: {int(train_x.shape[0]/batch_size)}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "XLft0abY1v9o"
      },
      "outputs": [],
      "source": [
        "# release some memory\n",
        "del train_x, train_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_UF2Rd91v9o"
      },
      "source": [
        "We can also check if we have any GPUs to speed up our training time by many folds. If you’re using \"https://colab.research.google.com/\" with GPU to run this code, the training time will be significantly reduced."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "owOlEW3c1v9o"
      },
      "outputs": [],
      "source": [
        "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c40ps2Xp1v9p"
      },
      "source": [
        "Next, we'll be defining the structure of the GRU and LSTM models. Both models have the same structure, with the only difference being the **recurrent layer** (GRU/LSTM) and the initializing of the hidden state. The hidden state for the LSTM is a tuple containing both the **cell state** and the **hidden state**, whereas the **GRU only has a single hidden state**.\n",
        "Please refer to official PyTorch documentation to get familiar with GRU and LSTM interfaces in PyTorch:\n",
        "\n",
        "- https://pytorch.org/docs/stable/nn.html#torch.nn.GRU\n",
        "- https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM\n",
        "\n",
        "You can also detailed tutorials about Recurrent Neural Networks on my [Blog](http://www.sefidian.com/archives/) and [Github](https://github.com/iamirmasoud)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "467NlRNw1v9p"
      },
      "source": [
        "# LSTM\n",
        "![lstm1.png](https://github.com/iamirmasoud/energy_consumption_prediction/blob/master/imgs/lstm1.png?raw=1)\n",
        "![lstm2.png](https://github.com/iamirmasoud/energy_consumption_prediction/blob/master/imgs/lstm2.png?raw=1)\n",
        "\n",
        "# GRU\n",
        "![gru1.png](https://github.com/iamirmasoud/energy_consumption_prediction/blob/master/imgs/gru1.png?raw=1)\n",
        "![gru2.png](https://github.com/iamirmasoud/energy_consumption_prediction/blob/master/imgs/gru2.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "CHTQWVaV1v9p"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pickle\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class GRUNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
        "        super(GRUNet, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.gru = nn.GRU(\n",
        "            input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x, h):\n",
        "        out, h = self.gru(x, h)\n",
        "        # print(out[:, -1].shape, h.shape)\n",
        "        # select hidden state of last timestamp (t=90) (1024, 256)\n",
        "        out = self.fc(self.relu(out[:, -1]))  # out[:, -1, :]\n",
        "        # print(out.shape) # (1024, 1)\n",
        "        return out, h\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        # Initialze h_0 with zeros\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = (\n",
        "            weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
        "        )\n",
        "        return hidden\n",
        "\n",
        "\n",
        "class LSTMNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
        "        super(LSTMNet, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x, h):\n",
        "        out, h = self.lstm(x, h)\n",
        "        out = self.fc(self.relu(out[:, -1]))\n",
        "        return out, h\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        # Initialze h_0, c_0 with zeros\n",
        "        hidden = (\n",
        "            weight.new(self.n_layers, batch_size, self.hidden_dim)\n",
        "            .zero_()\n",
        "            .to(device),  # h_0\n",
        "            weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
        "        )\n",
        "        return hidden\n",
        "\n",
        "\n",
        "class ARIMAModel:\n",
        "    def __init__(self, order=(1, 1, 1)):\n",
        "        \"\"\"\n",
        "        Initialize ARIMA model with specified order\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        order : tuple\n",
        "            (p, d, q) order of the ARIMA model\n",
        "            p: The number of lag observations (AR order)\n",
        "            d: The degree of differencing\n",
        "            q: The size of the moving average window (MA order)\n",
        "        \"\"\"\n",
        "        self.order = order\n",
        "        self.model = None\n",
        "        self.fitted_model = None\n",
        "\n",
        "    def fit(self, train_data):\n",
        "        \"\"\"\n",
        "        Fit ARIMA model to the training data\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        train_data : array-like\n",
        "            Time series data for training\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        self : returns an instance of self\n",
        "        \"\"\"\n",
        "        # Convert to pandas Series if not already\n",
        "        if not isinstance(train_data, pd.Series):\n",
        "            train_data = pd.Series(train_data)\n",
        "\n",
        "        # Fit ARIMA model\n",
        "        self.model = ARIMA(train_data, order=self.order)\n",
        "        self.fitted_model = self.model.fit()\n",
        "        return self\n",
        "\n",
        "    def predict(self, steps=1):\n",
        "        \"\"\"\n",
        "        Make predictions using the fitted model\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        steps : int\n",
        "            Number of steps to forecast ahead\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        predictions : array\n",
        "            Predicted values\n",
        "        \"\"\"\n",
        "        if self.fitted_model is None:\n",
        "            raise ValueError(\"Model has not been fitted yet. Call 'fit' first.\")\n",
        "\n",
        "        # Get forecast\n",
        "        forecast = self.fitted_model.forecast(steps=steps)\n",
        "        return forecast\n",
        "\n",
        "    def evaluate(self, test_data):\n",
        "        \"\"\"\n",
        "        Evaluate model performance on test data\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        test_data : array-like\n",
        "            Actual values to compare predictions against\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        mse : float\n",
        "            Mean squared error\n",
        "        \"\"\"\n",
        "        if self.fitted_model is None:\n",
        "            raise ValueError(\"Model has not been fitted yet. Call 'fit' first.\")\n",
        "\n",
        "        # Make predictions for test period\n",
        "        predictions = self.predict(steps=len(test_data))\n",
        "\n",
        "        # Calculate MSE\n",
        "        mse = mean_squared_error(test_data, predictions)\n",
        "        return mse\n",
        "\n",
        "    def summary(self):\n",
        "        \"\"\"\n",
        "        Return model summary information\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        summary : object\n",
        "            Summary of the fitted model\n",
        "        \"\"\"\n",
        "        if self.fitted_model is None:\n",
        "            raise ValueError(\"Model has not been fitted yet. Call 'fit' first.\")\n",
        "\n",
        "        return self.fitted_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ohVwhAZJ1v9p"
      },
      "outputs": [],
      "source": [
        "def train(\n",
        "    train_loader,\n",
        "    learn_rate,\n",
        "    hidden_dim=256,\n",
        "    n_layers=2,\n",
        "    n_epochs=5,\n",
        "    model_type=\"GRU\",\n",
        "    print_every=100,\n",
        "):\n",
        "\n",
        "    input_dim = next(iter(train_loader))[0].shape[2]  # 5\n",
        "\n",
        "    # Batch generator (train_data, train_label)\n",
        "    # print(next(iter(train_loader))[0].shape, next(iter(train_loader))[1].shape) # torch.Size([1024, 90, 5]) torch.Size([1024, 1])\n",
        "\n",
        "    output_dim = 1\n",
        "\n",
        "    # Instantiating the models\n",
        "    if model_type == \"GRU\":\n",
        "        model = GRUNet(input_dim, hidden_dim, output_dim, n_layers)\n",
        "    else:\n",
        "        model = LSTMNet(input_dim, hidden_dim, output_dim, n_layers)\n",
        "    model.to(device)\n",
        "\n",
        "    # Defining loss function and optimizer\n",
        "    criterion = nn.MSELoss()  # Mean Squared Error\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
        "\n",
        "    model.train()\n",
        "    print(\"Starting Training of {} model\".format(model_type))\n",
        "    epoch_times = []\n",
        "\n",
        "    # Start training loop\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        start_time = time.process_time()\n",
        "        h = model.init_hidden(batch_size)\n",
        "        avg_loss = 0.0\n",
        "        counter = 0\n",
        "        for x, label in train_loader:\n",
        "            counter += 1\n",
        "            if model_type == \"GRU\":\n",
        "                h = h.data\n",
        "            # Unpcak both h_0 and c_0\n",
        "            elif model_type == \"LSTM\":\n",
        "                h = tuple([e.data for e in h])\n",
        "\n",
        "            # Set the gradients to zero before starting to do backpropragation because\n",
        "            # PyTorch accumulates the gradients on subsequent backward passes\n",
        "            model.zero_grad()\n",
        "\n",
        "            out, h = model(x.to(device).float(), h)\n",
        "            loss = criterion(out, label.to(device).float())\n",
        "\n",
        "            # Perform backpropragation\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            avg_loss += loss.item()\n",
        "            if counter % print_every == 0:\n",
        "                print(\n",
        "                    f\"Epoch {epoch} - Step: {counter}/{len(train_loader)} - Average Loss for Epoch: {avg_loss/counter}\"\n",
        "                )\n",
        "        current_time = time.process_time()\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch}/{n_epochs} Done, Total Loss: {avg_loss/len(train_loader)}\"\n",
        "        )\n",
        "\n",
        "        print(f\"Time Elapsed for Epoch: {current_time-start_time} seconds\")\n",
        "\n",
        "        epoch_times.append(current_time - start_time)\n",
        "\n",
        "    print(f\"Total Training Time: {sum(epoch_times)} seconds\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_arima(\n",
        "    train_data,\n",
        "    test_data=None,\n",
        "    order=(1, 1, 1),\n",
        "    print_every=100\n",
        "):\n",
        "    \"\"\"\n",
        "    Train an ARIMA model on the provided data\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    train_data : array-like\n",
        "        Time series data for training\n",
        "    test_data : array-like, optional\n",
        "        Time series data for testing\n",
        "    order : tuple\n",
        "        (p, d, q) order of the ARIMA model\n",
        "    print_every : int\n",
        "        Print status update every this many samples\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    model : ARIMAModel\n",
        "        Trained ARIMA model\n",
        "    \"\"\"\n",
        "    print(\"Starting Training of ARIMA model\")\n",
        "    start_time = time.process_time()\n",
        "\n",
        "    # Convert data to the right format if needed\n",
        "    # If train_data is a tensor or DataFrame, convert to numpy array\n",
        "    if hasattr(train_data, 'numpy'):\n",
        "        train_data = train_data.numpy()\n",
        "\n",
        "    # For multivariate data, we need to extract the target variable\n",
        "    # Assuming the target is the last column/feature\n",
        "    if isinstance(train_data, np.ndarray) and train_data.ndim > 1:\n",
        "        if train_data.shape[1] > 1:\n",
        "            print(\"Note: ARIMA uses univariate data. Using the last column as target.\")\n",
        "            train_data = train_data[:, -1]\n",
        "\n",
        "    # Create and fit the model\n",
        "    model = ARIMAModel(order=order)\n",
        "\n",
        "    try:\n",
        "        print(f\"Fitting ARIMA{order} model...\")\n",
        "        model.fit(train_data)\n",
        "        print(\"Model fitting completed\")\n",
        "\n",
        "        # Print model summary\n",
        "        print(\"\\nModel Summary:\")\n",
        "        print(model.summary())\n",
        "\n",
        "        # Evaluate on test data if provided\n",
        "        if test_data is not None:\n",
        "            # Prepare test data if needed\n",
        "            if hasattr(test_data, 'numpy'):\n",
        "                test_data = test_data.numpy()\n",
        "\n",
        "            if isinstance(test_data, np.ndarray) and test_data.ndim > 1:\n",
        "                if test_data.shape[1] > 1:\n",
        "                    test_data = test_data[:, -1]\n",
        "\n",
        "            mse = model.evaluate(test_data)\n",
        "            print(f\"\\nTest MSE: {mse}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during model training: {e}\")\n",
        "\n",
        "    # Calculate and print training time\n",
        "    current_time = time.process_time()\n",
        "    training_time = current_time - start_time\n",
        "    print(f\"Total Training Time: {training_time} seconds\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the ARIMA model"
      ],
      "metadata": {
        "id": "GzcWdTqFDHXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the directory name\n",
        "model_dir = \"models\"\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "if not os.path.exists(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "    print(f\"Directory '{model_dir}' created successfully.\")\n",
        "else:\n",
        "    print(f\"Directory '{model_dir}' already exists.\")\n",
        "\n",
        "# Training the ARIMA model\n",
        "# seq_len = 90  # (timestamps)\n",
        "p = 5  # AR order\n",
        "d = 1  # Difference order\n",
        "q = 1  # MA order\n",
        "print_every = 100\n",
        "\n",
        "# Assuming you have some time series data\n",
        "# For ARIMA, we need to prepare a univariate time series\n",
        "# If your data is already in train_loader, you need to extract it\n",
        "\n",
        "# Example: Extract data from your existing loader\n",
        "# First, get a batch sample to understand structure\n",
        "sample_batch = next(iter(train_loader))\n",
        "sample_features = sample_batch[0].numpy()  # Shape: [batch_size, seq_len, n_features]\n",
        "sample_targets = sample_batch[1].numpy()   # Shape: [batch_size, 1]\n",
        "\n",
        "# For ARIMA, we need to flatten this to a single time series\n",
        "# This is just an example approach - adjust based on your actual data\n",
        "train_data_arima = []\n",
        "for batch_idx in range(len(train_loader)):\n",
        "    batch = next(iter(train_loader))\n",
        "    features = batch[0]\n",
        "    targets = batch[1]\n",
        "    # Add target values to our time series\n",
        "    train_data_arima.extend(targets.numpy().flatten())\n",
        "\n",
        "# Alternative: If using synthetic/sample data for testing\n",
        "# import numpy as np\n",
        "# np.random.seed(42)\n",
        "# train_data_arima = np.random.randn(1000)  # Example random data\n",
        "\n",
        "# Train the ARIMA model\n",
        "arima_model = train_arima(\n",
        "    train_data=train_data_arima,\n",
        "    order=(p, d, q),\n",
        "    print_every=print_every\n",
        ")\n",
        "\n",
        "# Save the ARIMA model\n",
        "with open(\"models/arima_model.pkl\", \"wb\") as f:\n",
        "    pickle.dump(arima_model, f)\n",
        "\n",
        "print(\"ARIMA model saved to models/arima_model.pkl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1Wy9mKjDPhC",
        "outputId": "1df0bd6c-f3a8-4e51-a471-873a8d2d9515"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory 'models' already exists.\n",
            "Starting Training of ARIMA model\n",
            "Fitting ARIMA(5, 1, 1) model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model fitting completed\n",
            "\n",
            "Model Summary:\n",
            "                               SARIMAX Results                                \n",
            "==============================================================================\n",
            "Dep. Variable:                      y   No. Observations:               429056\n",
            "Model:                 ARIMA(5, 1, 1)   Log Likelihood              166828.258\n",
            "Date:                Sun, 09 Mar 2025   AIC                        -333642.516\n",
            "Time:                        09:27:42   BIC                        -333565.730\n",
            "Sample:                             0   HQIC                       -333620.639\n",
            "                             - 429056                                         \n",
            "Covariance Type:                  opg                                         \n",
            "==============================================================================\n",
            "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "ar.L1         -0.0006      0.002     -0.416      0.677      -0.004       0.002\n",
            "ar.L2          0.0007      0.002      0.426      0.670      -0.002       0.004\n",
            "ar.L3          0.0011      0.002      0.693      0.488      -0.002       0.004\n",
            "ar.L4         -0.0010      0.002     -0.642      0.521      -0.004       0.002\n",
            "ar.L5          0.0001      0.002      0.093      0.926      -0.003       0.003\n",
            "ma.L1         -1.0000      0.000  -4412.785      0.000      -1.000      -1.000\n",
            "sigma2         0.0269   6.42e-05    419.076      0.000       0.027       0.027\n",
            "===================================================================================\n",
            "Ljung-Box (L1) (Q):                   0.00   Jarque-Bera (JB):              4792.19\n",
            "Prob(Q):                              0.96   Prob(JB):                         0.00\n",
            "Heteroskedasticity (H):               1.01   Skew:                             0.19\n",
            "Prob(H) (two-sided):                  0.29   Kurtosis:                         2.65\n",
            "===================================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n",
            "Total Training Time: 536.4000705489999 seconds\n",
            "ARIMA model saved to models/arima_model.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoRBXZCm1v9q"
      },
      "source": [
        "## Training the GRU model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPxXGUEV1v9q",
        "outputId": "90095efd-ee4f-4135-ad6d-ba03d61d6fa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Training of GRU model\n",
            "Epoch 1 - Step: 100/419 - Average Loss for Epoch: 0.010032557991798967\n",
            "Epoch 1 - Step: 200/419 - Average Loss for Epoch: 0.005623011600400787\n",
            "Epoch 1 - Step: 300/419 - Average Loss for Epoch: 0.004012226227399272\n",
            "Epoch 1 - Step: 400/419 - Average Loss for Epoch: 0.0031515160200069657\n",
            "Epoch 1/5 Done, Total Loss: 0.0030311964292369276\n",
            "Time Elapsed for Epoch: 3990.4072281649987 seconds\n",
            "Epoch 2 - Step: 100/419 - Average Loss for Epoch: 0.0004712287135771476\n",
            "Epoch 2 - Step: 200/419 - Average Loss for Epoch: 0.00042374114273115995\n",
            "Epoch 2 - Step: 300/419 - Average Loss for Epoch: 0.0003979211723587165\n",
            "Epoch 2 - Step: 400/419 - Average Loss for Epoch: 0.0003748137864386081\n",
            "Epoch 2/5 Done, Total Loss: 0.00036974055392485387\n",
            "Time Elapsed for Epoch: 3970.6969201939974 seconds\n",
            "Epoch 3 - Step: 100/419 - Average Loss for Epoch: 0.0002609270083485171\n",
            "Epoch 3 - Step: 200/419 - Average Loss for Epoch: 0.00026672345782571935\n",
            "Epoch 3 - Step: 300/419 - Average Loss for Epoch: 0.00025983965174721863\n",
            "Epoch 3 - Step: 400/419 - Average Loss for Epoch: 0.0002537226566346362\n",
            "Epoch 3/5 Done, Total Loss: 0.0002531397631455786\n",
            "Time Elapsed for Epoch: 3983.6664633889995 seconds\n",
            "Epoch 4 - Step: 100/419 - Average Loss for Epoch: 0.00022427149160648697\n",
            "Epoch 4 - Step: 200/419 - Average Loss for Epoch: 0.00022288562227913645\n",
            "Epoch 4 - Step: 300/419 - Average Loss for Epoch: 0.00022241861151997\n",
            "Epoch 4 - Step: 400/419 - Average Loss for Epoch: 0.00021877768198464765\n",
            "Epoch 4/5 Done, Total Loss: 0.00021763376427340924\n",
            "Time Elapsed for Epoch: 3992.602569270999 seconds\n",
            "Epoch 5 - Step: 100/419 - Average Loss for Epoch: 0.00018807799177011475\n",
            "Epoch 5 - Step: 200/419 - Average Loss for Epoch: 0.0001944151752104517\n",
            "Epoch 5 - Step: 300/419 - Average Loss for Epoch: 0.00019260575844479415\n",
            "Epoch 5 - Step: 400/419 - Average Loss for Epoch: 0.00018588761497085216\n",
            "Epoch 5/5 Done, Total Loss: 0.00018517262804397517\n",
            "Time Elapsed for Epoch: 3979.287448932002 seconds\n",
            "Total Training Time: 19916.660629950995 seconds\n"
          ]
        }
      ],
      "source": [
        "# seq_len = 90  # (timestamps)\n",
        "n_hidden = 256\n",
        "n_layers = 2\n",
        "n_epochs = 5\n",
        "print_every = 100\n",
        "lr = 0.001\n",
        "gru_model = train(\n",
        "    train_loader,\n",
        "    learn_rate=lr,\n",
        "    hidden_dim=n_hidden,\n",
        "    n_layers=n_layers,\n",
        "    n_epochs=n_epochs,\n",
        "    model_type=\"GRU\",\n",
        "    print_every=print_every,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qmlnfv7r1v9q"
      },
      "source": [
        "## Save the GRU model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "pibBx_121v9q"
      },
      "outputs": [],
      "source": [
        "torch.save(gru_model.state_dict(), \"./models/gru_model.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kChwWRnt1v9t"
      },
      "source": [
        "## Train and Save an LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOpBaIe21v9t",
        "outputId": "224f0495-d1e5-470e-bb90-4ef6d978b4aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Training of LSTM model\n",
            "Epoch 1 - Step: 100/419 - Average Loss for Epoch: 0.020150415431708098\n",
            "Epoch 1 - Step: 200/419 - Average Loss for Epoch: 0.011250900059239939\n"
          ]
        }
      ],
      "source": [
        "lstm_model = train(\n",
        "    train_loader,\n",
        "    learn_rate=lr,\n",
        "    hidden_dim=n_hidden,\n",
        "    n_layers=n_layers,\n",
        "    n_epochs=n_epochs,\n",
        "    model_type=\"LSTM\",\n",
        "    print_every=print_every,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcO-Uor91v9t"
      },
      "outputs": [],
      "source": [
        "torch.save(lstm_model.state_dict(), \"./models/lstm_model.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OunGihl-1v9u"
      },
      "source": [
        "As we can see from the training time of both models, the GRU model is the clear winner in terms of speed, as we have mentioned earlier. The GRU finished 5 training epochs faster than the LSTM model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the ARIMA model"
      ],
      "metadata": {
        "id": "_7m0zXuvHxC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# move device to cpu for evaluation to avoid GPU memory run\n",
        "device = \"cpu\"\n",
        "\n",
        "# Load the ARIMA model\n",
        "try:\n",
        "    with open(\"./models/arima_model.pkl\", \"rb\") as f:\n",
        "        arima_model = pickle.load(f)\n",
        "    print(\"ARIMA model loaded successfully\")\n",
        "except FileNotFoundError:\n",
        "    print(\"ARIMA model file not found. Please make sure the model has been trained and saved.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading ARIMA model: {e}\")\n",
        "\n",
        "# Display model information\n",
        "if 'arima_model' in locals():\n",
        "    print(f\"ARIMA model order: {arima_model.order}\")\n",
        "    print(\"Model summary:\")\n",
        "    print(arima_model.summary())"
      ],
      "metadata": {
        "id": "BSXqt0XWH3aR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WzcZq6o1v9u"
      },
      "source": [
        "## Load the GRU model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3ZRn9HV1v9u"
      },
      "outputs": [],
      "source": [
        "# move device to cpu for evaluation to avoid GPU memory run\n",
        "device = \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_dFioKL1v9u"
      },
      "outputs": [],
      "source": [
        "hidden_dim = 256\n",
        "input_dim = 5\n",
        "output_dim = 1\n",
        "n_layers = 2\n",
        "gru_model = GRUNet(input_dim, hidden_dim, output_dim, n_layers)\n",
        "gru_model.load_state_dict(torch.load(\"./models/gru_model.pt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQSRbGAw1v9u"
      },
      "outputs": [],
      "source": [
        "# Move the model to the appropriate device\n",
        "gru_model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uv9vbaER1v9u"
      },
      "source": [
        "## Load the LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8A_2uO0z1v9u"
      },
      "outputs": [],
      "source": [
        "hidden_dim = 256\n",
        "input_dim = 5\n",
        "output_dim = 1\n",
        "n_layers = 2\n",
        "lstm_model = LSTMNet(input_dim, hidden_dim, output_dim, n_layers)\n",
        "lstm_model.load_state_dict(torch.load(\"./models/lstm_model.pt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1woscFH1v9v"
      },
      "outputs": [],
      "source": [
        "# Move the model to the appropriate device\n",
        "lstm_model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzCJAtnG1v9v"
      },
      "source": [
        "## Model Evaluation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fknup-7J1v9v"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQfQqMp91v9v"
      },
      "outputs": [],
      "source": [
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate Mean Absolute Percentage Error (MAPE)\n",
        "\n",
        "    Args:\n",
        "        y_true: Actual values\n",
        "        y_pred: Predicted values\n",
        "\n",
        "    Returns:\n",
        "        MAPE value as percentage\n",
        "    \"\"\"\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    # Add small constant to avoid division by zero\n",
        "    epsilon = 1e-10\n",
        "    # Return MAPE as percentage\n",
        "    return 100 * np.mean(np.abs((y_true - y_pred) / (np.abs(y_true) + epsilon)))\n",
        "\n",
        "def evaluate_model(model_name, y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate and display various evaluation metrics for a model\n",
        "\n",
        "    Args:\n",
        "        model_name: Name of the model\n",
        "        y_true: Actual values\n",
        "        y_pred: Predicted values\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of evaluation metrics\n",
        "    \"\"\"\n",
        "    # Calculate metrics\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
        "\n",
        "    # Store metrics in dictionary\n",
        "    metrics = {\n",
        "        'Model': model_name,\n",
        "        'MSE': mse,\n",
        "        'RMSE': rmse,\n",
        "        'MAE': mae,\n",
        "        'R²': r2,\n",
        "        'MAPE (%)': mape\n",
        "    }\n",
        "\n",
        "    # Print metrics in a formatted way\n",
        "    print(f\"\\n----- {model_name} Model Evaluation -----\")\n",
        "    print(f\"Mean Squared Error (MSE): {mse:.6f}\")\n",
        "    print(f\"Root Mean Squared Error (RMSE): {rmse:.6f}\")\n",
        "    print(f\"Mean Absolute Error (MAE): {mae:.6f}\")\n",
        "    print(f\"R² Score: {r2:.6f}\")\n",
        "    print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def evaluate_gru_model(test_loader, gru_model, device='cpu'):\n",
        "    \"\"\"\n",
        "    Evaluate the GRU model on test data\n",
        "\n",
        "    Args:\n",
        "        test_loader: DataLoader containing test data\n",
        "        gru_model: Trained GRU model\n",
        "        device: Device to run evaluation on\n",
        "\n",
        "    Returns:\n",
        "        Evaluation metrics\n",
        "    \"\"\"\n",
        "    gru_model.eval()  # Set model to evaluation mode\n",
        "    y_true_list = []\n",
        "    y_pred_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs = inputs.to(device).float()\n",
        "            targets = targets.to(device).float()\n",
        "\n",
        "            # Initialize hidden state\n",
        "            h = gru_model.init_hidden(inputs.size(0))\n",
        "            h = h.data\n",
        "\n",
        "            # Forward pass\n",
        "            outputs, _ = gru_model(inputs, h)\n",
        "\n",
        "            # Store predictions and actual values\n",
        "            y_true_list.extend(targets.cpu().numpy().flatten())\n",
        "            y_pred_list.extend(outputs.cpu().numpy().flatten())\n",
        "\n",
        "    # Evaluate the model\n",
        "    metrics = evaluate_model('GRU', y_true_list, y_pred_list)\n",
        "\n",
        "    return metrics, y_true_list, y_pred_list\n",
        "\n",
        "def evaluate_lstm_model(test_loader, lstm_model, device='cpu'):\n",
        "    \"\"\"\n",
        "    Evaluate the LSTM model on test data\n",
        "\n",
        "    Args:\n",
        "        test_loader: DataLoader containing test data\n",
        "        lstm_model: Trained LSTM model\n",
        "        device: Device to run evaluation on\n",
        "\n",
        "    Returns:\n",
        "        Evaluation metrics\n",
        "    \"\"\"\n",
        "    lstm_model.eval()  # Set model to evaluation mode\n",
        "    y_true_list = []\n",
        "    y_pred_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs = inputs.to(device).float()\n",
        "            targets = targets.to(device).float()\n",
        "\n",
        "            # Initialize hidden state\n",
        "            h = lstm_model.init_hidden(inputs.size(0))\n",
        "            h = tuple([e.data for e in h])\n",
        "\n",
        "            # Forward pass\n",
        "            outputs, _ = lstm_model(inputs, h)\n",
        "\n",
        "            # Store predictions and actual values\n",
        "            y_true_list.extend(targets.cpu().numpy().flatten())\n",
        "            y_pred_list.extend(outputs.cpu().numpy().flatten())\n",
        "\n",
        "    # Evaluate the model\n",
        "    metrics = evaluate_model('LSTM', y_true_list, y_pred_list)\n",
        "\n",
        "    return metrics, y_true_list, y_pred_list\n",
        "\n",
        "def evaluate_arima_model(test_data, arima_model):\n",
        "    \"\"\"\n",
        "    Evaluate the ARIMA model on test data\n",
        "\n",
        "    Args:\n",
        "        test_data: Test data (array or Series)\n",
        "        arima_model: Trained ARIMA model\n",
        "\n",
        "    Returns:\n",
        "        Evaluation metrics\n",
        "    \"\"\"\n",
        "    # Ensure test_data is in the right format\n",
        "    if isinstance(test_data, torch.Tensor):\n",
        "        test_data = test_data.numpy()\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = arima_model.predict(steps=len(test_data))\n",
        "\n",
        "    # Evaluate the model\n",
        "    metrics = evaluate_model('ARIMA', test_data, predictions)\n",
        "\n",
        "    return metrics, test_data, predictions\n",
        "\n",
        "def compare_models(metrics_list):\n",
        "    \"\"\"\n",
        "    Compare different models based on their evaluation metrics\n",
        "\n",
        "    Args:\n",
        "        metrics_list: List of dictionaries containing evaluation metrics for different models\n",
        "    \"\"\"\n",
        "    # Create DataFrame from metrics\n",
        "    df = pd.DataFrame(metrics_list)\n",
        "\n",
        "    # Set Model as index for better display\n",
        "    df = df.set_index('Model')\n",
        "\n",
        "    # Display the comparison table\n",
        "    print(\"\\n----- Model Comparison -----\")\n",
        "    print(df)\n",
        "\n",
        "    # Create bar chart for MSE comparison\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Plot MSE\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.bar(df.index, df['MSE'])\n",
        "    plt.title('Mean Squared Error (MSE)')\n",
        "    plt.ylabel('MSE')\n",
        "\n",
        "    # Plot MAE\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.bar(df.index, df['MAE'])\n",
        "    plt.title('Mean Absolute Error (MAE)')\n",
        "    plt.ylabel('MAE')\n",
        "\n",
        "    # Plot R²\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.bar(df.index, df['R²'])\n",
        "    plt.title('R² Score')\n",
        "    plt.ylabel('R²')\n",
        "\n",
        "    # Plot MAPE\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.bar(df.index, df['MAPE (%)'])\n",
        "    plt.title('Mean Absolute Percentage Error (MAPE)')\n",
        "    plt.ylabel('MAPE (%)')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Determine the best model for each metric\n",
        "    best_mse = df['MSE'].idxmin()\n",
        "    best_mae = df['MAE'].idxmin()\n",
        "    best_r2 = df['R²'].idxmax()\n",
        "    best_mape = df['MAPE (%)'].idxmin()\n",
        "\n",
        "    print(\"\\n----- Best Models -----\")\n",
        "    print(f\"Best model according to MSE: {best_mse}\")\n",
        "    print(f\"Best model according to MAE: {best_mae}\")\n",
        "    print(f\"Best model according to R²: {best_r2}\")\n",
        "    print(f\"Best model according to MAPE: {best_mape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate performance of ARIMA"
      ],
      "metadata": {
        "id": "dN3V95gPIuAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arima_metrics, arima_true, arima_pred = evaluate_arima_model(test_data_arima, arima_model)"
      ],
      "metadata": {
        "id": "BJG_nMxTIwQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3B39iATX1v9v"
      },
      "source": [
        "## Evaluate performance of GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgHbvYn11v9v"
      },
      "outputs": [],
      "source": [
        "gru_metrics, gru_true, gru_pred = evaluate_gru_model(test_loader, gru_model, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65mNvDrk1v9w"
      },
      "source": [
        "## Evaluate performance of LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_q296Kb1v9w"
      },
      "outputs": [],
      "source": [
        "lstm_metrics, lstm_true, lstm_pred = evaluate_lstm_model(test_loader, lstm_model, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NS4gp1Cn1v9w"
      },
      "source": [
        "Comparing all the 3 models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLWAi3Mc1v9w"
      },
      "outputs": [],
      "source": [
        "all_metrics = [gru_metrics, lstm_metrics, arima_metrics]\n",
        "compare_models(all_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u899zSuJ1v9w"
      },
      "source": [
        "# Some visualizations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhcmtv2j1v9w"
      },
      "source": [
        "Lastly, let's do some visualizations on random sets of our predicted output vs the actual consumption data for some states."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQQDvL0r1v9w"
      },
      "outputs": [],
      "source": [
        "states_list = list(test_x.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxlQV7dE1v9x"
      },
      "outputs": [],
      "source": [
        "states_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJm4ick51v9x"
      },
      "outputs": [],
      "source": [
        "# Plot predictions vs actual values for each model\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Plot for GRU\n",
        "plt.subplot(3, 1, 1)\n",
        "plt.plot(gru_true[:100], label='Actual')\n",
        "plt.plot(gru_pred[:100], label='Predicted')\n",
        "plt.title('GRU: Actual vs Predicted')\n",
        "plt.legend()\n",
        "\n",
        "# Plot for LSTM\n",
        "plt.subplot(3, 1, 2)\n",
        "plt.plot(lstm_true[:100], label='Actual')\n",
        "plt.plot(lstm_pred[:100], label='Predicted')\n",
        "plt.title('LSTM: Actual vs Predicted')\n",
        "plt.legend()\n",
        "\n",
        "# Plot for ARIMA\n",
        "plt.subplot(3, 1, 3)\n",
        "plt.plot(arima_true[:100], label='Actual')\n",
        "plt.plot(arima_pred[:100], label='Predicted')\n",
        "plt.title('ARIMA: Actual vs Predicted')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbdvDoQb1v9x"
      },
      "source": [
        "Looks like the models are largely successful in predicting the trends of energy consumption. While they may still get some changes wrong, such as delays in predicting a drop in consumption, the predictions follow very closely to the actual line on the test set. This is due to the nature of energy consumption data and the fact that there are patterns and cyclical changes that the model can account for. Tougher time-series prediction problems such as stock price prediction or sales volume prediction may have data that is largely random or doesn’t have predictable patterns, and in such cases, the accuracy will definitely be lower."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEPR8iCT1v9x"
      },
      "source": [
        "## What's next?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZw6T7tr1v9x"
      },
      "source": [
        "* Use more data.\n",
        "* Use more complex (more layers) networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgVqyoeL1v9x"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-0uwAN21v9x"
      },
      "source": [
        "Referenced from https://blog.floydhub.com/gru-with-pytorch/"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ebe6b66656b84c7f8bb2555888b4cf0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_070d0424f08c45c19d91419307d05b9d",
              "IPY_MODEL_bcdf2cccc4bc4f0fa352e9393a530b74",
              "IPY_MODEL_69810c5b35174d36b4051ae8bd9fe09b"
            ],
            "layout": "IPY_MODEL_ccc40be9df774338811495900cd07804"
          }
        },
        "070d0424f08c45c19d91419307d05b9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3bb9fba394c8453a80ced5c821a133b5",
            "placeholder": "​",
            "style": "IPY_MODEL_8ca3a5f8262745d9aae7b540fd6a91ec",
            "value": "100%"
          }
        },
        "bcdf2cccc4bc4f0fa352e9393a530b74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eef400e09ea342dfb7e1011dc45bb4a8",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b73b3fc743ae42aca4c14b3b3ef0bdc6",
            "value": 5
          }
        },
        "69810c5b35174d36b4051ae8bd9fe09b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e636b940965646bfb7f4c6ff3d58df50",
            "placeholder": "​",
            "style": "IPY_MODEL_7b7b8c041d3e4f22bc01212a0212b241",
            "value": " 5/5 [00:26&lt;00:00,  5.59s/it]"
          }
        },
        "ccc40be9df774338811495900cd07804": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3bb9fba394c8453a80ced5c821a133b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ca3a5f8262745d9aae7b540fd6a91ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eef400e09ea342dfb7e1011dc45bb4a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b73b3fc743ae42aca4c14b3b3ef0bdc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e636b940965646bfb7f4c6ff3d58df50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b7b8c041d3e4f22bc01212a0212b241": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}