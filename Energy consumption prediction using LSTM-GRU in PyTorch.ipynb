{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YasrajDhal/Electricity-Demand-Prediction/blob/main/Energy%20consumption%20prediction%20using%20LSTM-GRU%20in%20PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aazZxwlcXgb"
      },
      "source": [
        "# Energy consumption prediction using LSTM/GRU in PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWq9xyXKcXgh"
      },
      "source": [
        "In this notebook, we'll be using GRU and LSTM models for a time series prediction task and we will compare the performance of the GRU model against an LSTM model as well. The dataset that we will be using is the Hourly Energy Consumption dataset which can be found on [Kaggle](https://www.kaggle.com/robikscube/hourly-energy-consumption). The dataset contains power consumption data across different regions around the United States recorded on an hourly basis.\n",
        "\n",
        "Link to the GitHub repository: https://github.com/amirmasoudsfd/energy-consumption-prediction.\n",
        "\n",
        "The goal of this implementation is to **create a model that can accurately predict the energy usage in the next hour** given historical usage data. We will be using both the GRU and LSTM model to train on a set of historical data and evaluate both models on an unseen test set. To do so, we'll start with feature selection, data-preprocessing, followed by defining, training and eventually evaluating the models.\n",
        "\n",
        "We will use the PyTorch library to implement both types of models along with other common Python libraries used in data analytics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "357AC85kcXgi"
      },
      "source": [
        "## Important Points:\n",
        "* Basic knowledge of Deep Learning, Python, Numpy, Pandas, and Pytorch is required.\n",
        "* We will use LSTM/GRU units as black boxes. Internals are explained here:\n",
        "\n",
        "    * https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
        "\n",
        "    * http://www.sefidian.com/2020/01/30/gated-recurrent-unit-gru-with-pytorch/\n",
        "\n",
        "* No need to GPU. For high speed execution I recommend you to use [GOOGLE COLAB](https://colab.research.google.com) if your hardware is not powerful enough."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQtrhXiwcXgj"
      },
      "source": [
        "## GRU/LSTM cells"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDaVDbJrcXgj"
      },
      "source": [
        "* Long Short-Term Memory networks (LSTMs) have great memories and can remember information which the vanilla RNN is unable to!\n",
        "\n",
        "* The Gated Recurrent Unit (GRU) is the younger sibling of the more popular Long Short-Term Memory (LSTM) network, and also a type of Recurrent Neural Network (RNN). Just like its sibling, GRUs are able to effectively retain long-term dependencies in sequential data. And additionally, they can address the “short-term memory” issue plaguing vanilla RNNs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnZrPlUucXgk"
      },
      "source": [
        "![gru.jpg](attachment:gru.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CucDcu2vcXgl"
      },
      "source": [
        "![lstm-vs-gru.jpg](attachment:lstm-vs-gru.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHjlA8iycXgl"
      },
      "source": [
        "## The ML Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaPcMKBScXgm"
      },
      "source": [
        "![ml%20pipeline.jpg](attachment:ml%20pipeline.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AsXA5Y4acXgn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "from tqdm.notebook import tqdm as tqdm_notebook\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NTpziMDNcXgp",
        "outputId": "09400bac-b81a-40ca-b53d-a290569cdd72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5.1+cu124\n"
          ]
        }
      ],
      "source": [
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdBHHku-cXgq"
      },
      "source": [
        "## Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "InzFEtiUcXgr",
        "outputId": "7996235a-8498-4d94-8368-71b0ab816623",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 847
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.6.17)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.3.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.11/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle) (3.10)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5eadc4fb-9976-4e94-ba31-30c7580a4ab7\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5eadc4fb-9976-4e94-ba31-30c7580a4ab7\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving AEP_hourly.csv to AEP_hourly.csv\n",
            "Saving COMED_hourly.csv to COMED_hourly.csv\n",
            "Saving DAYTON_hourly.csv to DAYTON_hourly.csv\n",
            "Saving DEOK_hourly.csv to DEOK_hourly.csv\n",
            "Saving DOM_hourly.csv to DOM_hourly.csv\n",
            "Saving DUQ_hourly.csv to DUQ_hourly.csv\n",
            "Saving EKPC_hourly.csv to EKPC_hourly.csv\n",
            "Saving FE_hourly.csv to FE_hourly.csv\n",
            "Saving NI_hourly.csv to NI_hourly.csv\n",
            "Saving PJM_Load_hourly.csv to PJM_Load_hourly.csv\n",
            "Saving PJME_hourly.csv to PJME_hourly.csv\n",
            "Saving PJMW_hourly.csv to PJMW_hourly.csv\n",
            "cp: cannot stat 'kaggle.json': No such file or directory\n",
            "chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n",
            "Dataset URL: https://www.kaggle.com/datasets/robikscube/hourly-energy-consumption\n",
            "License(s): CC0-1.0\n",
            "Downloading hourly-energy-consumption.zip to /content/data\n",
            "  0% 0.00/11.4M [00:00<?, ?B/s]\n",
            "100% 11.4M/11.4M [00:00<00:00, 148MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Define data root directory\n",
        "!pip install kaggle\n",
        "from google.colab import files\n",
        "files.upload() # Upload your kaggle.json here\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d robikscube/hourly-energy-consumption -p /content/data --unzip\n",
        "\n",
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "from tqdm.notebook import tqdm as tqdm_notebook\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Update data_dir to point to the downloaded dataset\n",
        "data_dir = '/content/data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hERwNM1vcXgr"
      },
      "outputs": [],
      "source": [
        "pd.read_csv(os.path.join(data_dir, \"DEOK_hourly.csv\")).head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBvg4xjmcXgs"
      },
      "source": [
        "We have a total of **12** *.csv* files containing hourly energy trend data (*'est_hourly.paruqet'* and *'pjm_hourly_est.csv'* are not used). In our next step, we will be reading these files and pre-processing these data in this order:\n",
        "- Getting the time data of each individual time step and generalizing them\n",
        "    - Hour of the day *i.e. 0-23*\n",
        "    - Day of the week *i.e. 1-7*\n",
        "    - Month *i.e. 1-12*\n",
        "    - Day of the year *i.e. 1-365*\n",
        "    \n",
        "    \n",
        "- Scale the data to values between 0 and 1\n",
        "    - Algorithms tend to perform better or converge faster when features are on a relatively similar scale and/or close to normally distributed\n",
        "    - Scaling preserves the shape of the original distribution and doesn't reduce the importance of outliers\n",
        "    \n",
        "    \n",
        "- Group the data into sequences to be used as inputs to the model and store their corresponding labels\n",
        "    - The **sequence length** or **window_size period** is the number of data points in history that the model will use to make the prediction\n",
        "    - The label will be the next data point in time after the last one in the input sequence\n",
        "    \n",
        "\n",
        "- The inputs and labels will then be split into training and test sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlXVXE97cXgs"
      },
      "source": [
        "## Create training instances by moving sliding window"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKYrJ9VycXgs"
      },
      "outputs": [],
      "source": [
        "def move_sliding_window(data, window_size, inputs_cols_indices, label_col_index):\n",
        "    \"\"\"\n",
        "    data: numpy array including data\n",
        "    window_size: size of window\n",
        "    inputs_cols_indices: col indices to include\n",
        "    \"\"\"\n",
        "\n",
        "    # (# instances created by movement, seq_len (timestamps), # features (input_len))\n",
        "    inputs = np.zeros((len(data) - window_size, window_size, len(inputs_cols_indices)))\n",
        "    labels = np.zeros(len(data) - window_size)\n",
        "\n",
        "    for i in range(window_size, len(data)):\n",
        "        inputs[i - window_size] = data[i - window_size : i, inputs_cols_indices]\n",
        "        labels[i - window_size] = data[i, label_col_index]\n",
        "    inputs = inputs.reshape(-1, window_size, len(inputs_cols_indices))\n",
        "    labels = labels.reshape(-1, 1)\n",
        "    print(inputs.shape, labels.shape)\n",
        "\n",
        "    return inputs, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUXNqCbPcXgt"
      },
      "source": [
        "![window.jpg](attachment:window.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKIw1JoncXgt"
      },
      "source": [
        "![lstm-unrolled.png](attachment:lstm-unrolled.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8ieASt6cXgt"
      },
      "source": [
        "## Integrate files to build the training set\n",
        "To speed things up, I will only be using `num_files_for_dataset` .csv files for creating my dataset. Feel free to run it yourself with the entire dataset if you have the time and computing capacity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96mjkgKIcXgu"
      },
      "outputs": [],
      "source": [
        "label_col_index = 0  # consumption as label to predict\n",
        "inputs_cols_indices = range(\n",
        "    5\n",
        ")  # use (consumption, hour, dayofweek, month, dayofyear) columns as features\n",
        "\n",
        "# Define window_size period and split inputs/labels\n",
        "window_size = 90\n",
        "\n",
        "# The scaler objects will be stored in this dictionary so that our output test data from the model can be re-scaled during evaluation\n",
        "label_scalers = {}\n",
        "\n",
        "train_x = []\n",
        "test_x = {}\n",
        "test_y = {}\n",
        "\n",
        "# Skipping the files we're not using\n",
        "processing_files = [\n",
        "    file for file in os.listdir(data_dir) if os.path.splitext(file)[1] == \".csv\"\n",
        "]\n",
        "\n",
        "num_files_for_dataset = 5\n",
        "\n",
        "for file in tqdm_notebook(processing_files[:num_files_for_dataset]):\n",
        "    print(f\"Processing {file} ...\")\n",
        "    # Store csv file in a Pandas DataFrame\n",
        "    df = pd.read_csv(os.path.join(data_dir, file), parse_dates=[\"Datetime\"])\n",
        "\n",
        "    # Processing the time data into suitable input formats\n",
        "    df[\"hour\"] = df.apply(lambda x: x[\"Datetime\"].hour, axis=1)\n",
        "    df[\"dayofweek\"] = df.apply(lambda x: x[\"Datetime\"].dayofweek, axis=1)\n",
        "    df[\"month\"] = df.apply(lambda x: x[\"Datetime\"].month, axis=1)\n",
        "    df[\"dayofyear\"] = df.apply(lambda x: x[\"Datetime\"].dayofyear, axis=1)\n",
        "    df = df.sort_values(\"Datetime\").drop(\"Datetime\", axis=1)\n",
        "\n",
        "    # Scaling the input data\n",
        "    sc = MinMaxScaler()\n",
        "    label_sc = MinMaxScaler()\n",
        "    data = sc.fit_transform(df.values)\n",
        "\n",
        "    # Obtaining the scaler for the labels(usage data) so that output can be\n",
        "    # re-scaled to actual value during evaluation\n",
        "    label_sc.fit(df.iloc[:, label_col_index].values.reshape(-1, 1))\n",
        "    label_scalers[file] = label_sc\n",
        "\n",
        "    # Move the window\n",
        "    inputs, labels = move_sliding_window(\n",
        "        data,\n",
        "        window_size,\n",
        "        inputs_cols_indices=inputs_cols_indices,\n",
        "        label_col_index=label_col_index,\n",
        "    )\n",
        "\n",
        "    # CONCAT created instances from all .csv files.\n",
        "    # Split data into train/test portions and combining all data from different files into a single array\n",
        "    test_portion = int(0.1 * len(inputs))\n",
        "    if len(train_x) == 0:  # first iteration\n",
        "        train_x = inputs[:-test_portion]\n",
        "        train_y = labels[:-test_portion]\n",
        "    else:\n",
        "        train_x = np.concatenate((train_x, inputs[:-test_portion]))\n",
        "        train_y = np.concatenate((train_y, labels[:-test_portion]))\n",
        "    test_x[file] = inputs[-test_portion:]\n",
        "    test_y[file] = labels[-test_portion:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kG2HLJzcXgv"
      },
      "source": [
        "![input_shape.png](https://github.com/iamirmasoud/energy_consumption_prediction/blob/master/imgs/input_shape.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uh9aFDq_cXgv"
      },
      "source": [
        "## What have we made?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6OmPxkGcXgv"
      },
      "outputs": [],
      "source": [
        "train_x.shape, test_x[\"DEOK_hourly.csv\"].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Z-KPppfcXgv"
      },
      "source": [
        "## Pytorch data loaders/generators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4qhQo0IcXgv"
      },
      "source": [
        "To improve the speed of our training, we can process the data in batches so that the model does not need to update its weights as frequently. The `TensorDataset` and `DataLoader` classes are useful for splitting our data into batches and shuffling them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KP1gmdI-cXgw"
      },
      "outputs": [],
      "source": [
        "batch_size = 1024\n",
        "\n",
        "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "\n",
        "# Drop the last incomplete batch\n",
        "train_loader = DataLoader(\n",
        "    train_data, shuffle=True, batch_size=batch_size, drop_last=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aqab83lfcXgw"
      },
      "outputs": [],
      "source": [
        "print(\n",
        "    f\"Train Size: {train_x.shape}, Batch Size: {batch_size}, # of iterations per epoch: {int(train_x.shape[0]/batch_size)}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRDJD3oTcXgw"
      },
      "outputs": [],
      "source": [
        "# release some memory\n",
        "del train_x, train_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLs1ImNpcXgx"
      },
      "source": [
        "We can also check if we have any GPUs to speed up our training time by many folds. If you’re using \"https://colab.research.google.com/\" with GPU to run this code, the training time will be significantly reduced."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFhvnN-_cXgx"
      },
      "outputs": [],
      "source": [
        "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4zRajy1cXgx"
      },
      "source": [
        "Next, we'll be defining the structure of the GRU and LSTM models. Both models have the same structure, with the only difference being the **recurrent layer** (GRU/LSTM) and the initializing of the hidden state. The hidden state for the LSTM is a tuple containing both the **cell state** and the **hidden state**, whereas the **GRU only has a single hidden state**.\n",
        "Please refer to official PyTorch documentation to get familiar with GRU and LSTM interfaces in PyTorch:\n",
        "\n",
        "- https://pytorch.org/docs/stable/nn.html#torch.nn.GRU\n",
        "- https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM\n",
        "\n",
        "You can also detailed tutorials about Recurrent Neural Networks on my [Blog](http://www.sefidian.com/archives/) and [Github](https://github.com/iamirmasoud)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7Kpkn7EcXgy"
      },
      "source": [
        "# LSTM\n",
        "![lstm1.png](https://github.com/iamirmasoud/energy_consumption_prediction/blob/master/imgs/lstm1.png?raw=1)\n",
        "![lstm2.png](https://github.com/iamirmasoud/energy_consumption_prediction/blob/master/imgs/lstm2.png?raw=1)\n",
        "\n",
        "# GRU\n",
        "![gru1.png](https://github.com/iamirmasoud/energy_consumption_prediction/blob/master/imgs/gru1.png?raw=1)\n",
        "![gru2.png](https://github.com/iamirmasoud/energy_consumption_prediction/blob/master/imgs/gru2.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxcVt5YzcXgy"
      },
      "outputs": [],
      "source": [
        "class GRUNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
        "        super(GRUNet, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.gru = nn.GRU(\n",
        "            input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x, h):\n",
        "        out, h = self.gru(x, h)\n",
        "        # print(out[:, -1].shape, h.shape)\n",
        "        # select hidden state of last timestamp (t=90) (1024, 256)\n",
        "        out = self.fc(self.relu(out[:, -1]))  # out[:, -1, :]\n",
        "        # print(out.shape) # (1024, 1)\n",
        "        return out, h\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        # Initialze h_0 with zeros\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = (\n",
        "            weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
        "        )\n",
        "        return hidden\n",
        "\n",
        "\n",
        "class LSTMNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
        "        super(LSTMNet, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x, h):\n",
        "        out, h = self.lstm(x, h)\n",
        "        out = self.fc(self.relu(out[:, -1]))\n",
        "        return out, h\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        # Initialze h_0, c_0 with zeros\n",
        "        hidden = (\n",
        "            weight.new(self.n_layers, batch_size, self.hidden_dim)\n",
        "            .zero_()\n",
        "            .to(device),  # h_0\n",
        "            weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
        "        )\n",
        "        return hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZAnJH67cXgz"
      },
      "outputs": [],
      "source": [
        "def train(\n",
        "    train_loader,\n",
        "    learn_rate,\n",
        "    hidden_dim=256,\n",
        "    n_layers=2,\n",
        "    n_epochs=5,\n",
        "    model_type=\"GRU\",\n",
        "    print_every=100,\n",
        "):\n",
        "\n",
        "    input_dim = next(iter(train_loader))[0].shape[2]  # 5\n",
        "\n",
        "    # Batch generator (train_data, train_label)\n",
        "    # print(next(iter(train_loader))[0].shape, next(iter(train_loader))[1].shape) # torch.Size([1024, 90, 5]) torch.Size([1024, 1])\n",
        "\n",
        "    output_dim = 1\n",
        "\n",
        "    # Instantiating the models\n",
        "    if model_type == \"GRU\":\n",
        "        model = GRUNet(input_dim, hidden_dim, output_dim, n_layers)\n",
        "    else:\n",
        "        model = LSTMNet(input_dim, hidden_dim, output_dim, n_layers)\n",
        "    model.to(device)\n",
        "\n",
        "    # Defining loss function and optimizer\n",
        "    criterion = nn.MSELoss()  # Mean Squared Error\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
        "\n",
        "    model.train()\n",
        "    print(\"Starting Training of {} model\".format(model_type))\n",
        "    epoch_times = []\n",
        "\n",
        "    # Start training loop\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        start_time = time.process_time()\n",
        "        h = model.init_hidden(batch_size)\n",
        "        avg_loss = 0.0\n",
        "        counter = 0\n",
        "        for x, label in train_loader:\n",
        "            counter += 1\n",
        "            if model_type == \"GRU\":\n",
        "                h = h.data\n",
        "            # Unpcak both h_0 and c_0\n",
        "            elif model_type == \"LSTM\":\n",
        "                h = tuple([e.data for e in h])\n",
        "\n",
        "            # Set the gradients to zero before starting to do backpropragation because\n",
        "            # PyTorch accumulates the gradients on subsequent backward passes\n",
        "            model.zero_grad()\n",
        "\n",
        "            out, h = model(x.to(device).float(), h)\n",
        "            loss = criterion(out, label.to(device).float())\n",
        "\n",
        "            # Perform backpropragation\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            avg_loss += loss.item()\n",
        "            if counter % print_every == 0:\n",
        "                print(\n",
        "                    f\"Epoch {epoch} - Step: {counter}/{len(train_loader)} - Average Loss for Epoch: {avg_loss/counter}\"\n",
        "                )\n",
        "        current_time = time.process_time()\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch}/{n_epochs} Done, Total Loss: {avg_loss/len(train_loader)}\"\n",
        "        )\n",
        "\n",
        "        print(f\"Time Elapsed for Epoch: {current_time-start_time} seconds\")\n",
        "\n",
        "        epoch_times.append(current_time - start_time)\n",
        "\n",
        "    print(f\"Total Training Time: {sum(epoch_times)} seconds\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orkyvrf6cXgz"
      },
      "source": [
        "## Training the GRU model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "fuuz42kgcXg0"
      },
      "outputs": [],
      "source": [
        "# seq_len = 90  # (timestamps)\n",
        "n_hidden = 256\n",
        "n_layers = 2\n",
        "n_epochs = 5\n",
        "print_every = 100\n",
        "lr = 0.001\n",
        "gru_model = train(\n",
        "    train_loader,\n",
        "    learn_rate=lr,\n",
        "    hidden_dim=n_hidden,\n",
        "    n_layers=n_layers,\n",
        "    n_epochs=n_epochs,\n",
        "    model_type=\"GRU\",\n",
        "    print_every=print_every,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eX2-Y9C7cXg0"
      },
      "source": [
        "## Save the GRU model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXuP5pibcXg0"
      },
      "outputs": [],
      "source": [
        "torch.save(gru_model.state_dict(), \"./models/gru_model.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsa7Ua4WcXg6"
      },
      "source": [
        "## Train and Save an LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0v4aTG1cXg7"
      },
      "outputs": [],
      "source": [
        "lstm_model = train(\n",
        "    train_loader,\n",
        "    learn_rate=lr,\n",
        "    hidden_dim=n_hidden,\n",
        "    n_layers=n_layers,\n",
        "    n_epochs=n_epochs,\n",
        "    model_type=\"LSTM\",\n",
        "    print_every=print_every,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJ2n-bk4cXg7"
      },
      "outputs": [],
      "source": [
        "torch.save(lstm_model.state_dict(), \"./models/lstm_model.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyoiMXjBcXg7"
      },
      "source": [
        "As we can see from the training time of both models, the GRU model is the clear winner in terms of speed, as we have mentioned earlier. The GRU finished 5 training epochs faster than the LSTM model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9lxd9OlcXg8"
      },
      "source": [
        "# Evaluating models\n",
        "#### __Note: Running the following codes needs at least 16GB of memory.__\n",
        "Moving on to measuring the accuracy of both models, we'll now use our `evaluate()` function and test dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfEKb_j-cXg8"
      },
      "source": [
        "## Load the GRU model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVI7nO1NcXg8"
      },
      "outputs": [],
      "source": [
        "# move device to cpu for evaluation to avoid GPU memory run\n",
        "device = \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCOENIjHcXg8"
      },
      "outputs": [],
      "source": [
        "hidden_dim = 256\n",
        "input_dim = 5\n",
        "output_dim = 1\n",
        "n_layers = 2\n",
        "gru_model = GRUNet(input_dim, hidden_dim, output_dim, n_layers)\n",
        "gru_model.load_state_dict(torch.load(\"./models/gru_model.pt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAecFS5icXg9"
      },
      "outputs": [],
      "source": [
        "# Move the model to the appropriate device\n",
        "gru_model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4Zgf-EQcXg9"
      },
      "source": [
        "## Load the LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwuiGOuOcXg9"
      },
      "outputs": [],
      "source": [
        "hidden_dim = 256\n",
        "input_dim = 5\n",
        "output_dim = 1\n",
        "n_layers = 2\n",
        "lstm_model = LSTMNet(input_dim, hidden_dim, output_dim, n_layers)\n",
        "lstm_model.load_state_dict(torch.load(\"./models/lstm_model.pt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAHczwibcXg-"
      },
      "outputs": [],
      "source": [
        "# Move the model to the appropriate device\n",
        "lstm_model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZ9Mc9x-cXg-"
      },
      "source": [
        "## Model Evaluation\n",
        "\n",
        "For the purpose of comparing the performance of both models as well, we'll being tracking the time it takes for the model to train and eventually comparing the final accuracy of both models on the test set. For our accuracy measure, we'll use ***Symmetric Mean Absolute Percentage Error (sMAPE)*** to evaluate the models. *sMAPE* is the sum of the **absolute difference** between the predicted and actual values divided by the average of the predicted and actual value, therefore giving a percentage measuring the amount of error.\n",
        "\n",
        "This is the formula for *sMAPE*:\n",
        "\n",
        "$sMAPE = \\frac{100%}{n} \\sum_{t=1}^n \\frac{|F_t - A_t|}{(|F_t + A_t|)/2}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_k51JGO0cXg-"
      },
      "outputs": [],
      "source": [
        "def sMAPE(outputs, targets):\n",
        "    sMAPE = (\n",
        "        100\n",
        "        / len(targets)\n",
        "        * np.sum(np.abs(outputs - targets) / (np.abs(outputs + targets)) / 2)\n",
        "    )\n",
        "    return sMAPE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06vW8EOCcXg_"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, test_x, test_y, label_scalers):\n",
        "    model.eval()\n",
        "    outputs = []\n",
        "    targets = []\n",
        "    start_time = time.process_time()\n",
        "    # get data of test data for each state\n",
        "    for file in test_x.keys():\n",
        "        inputs = torch.from_numpy(np.array(test_x[file]))\n",
        "        labels = torch.from_numpy(np.array(test_y[file]))\n",
        "\n",
        "        h = model.init_hidden(inputs.shape[0])\n",
        "\n",
        "        # predict outputs\n",
        "        with torch.no_grad():\n",
        "            out, h = model(inputs.to(device).float(), h)\n",
        "\n",
        "        outputs.append(\n",
        "            label_scalers[file]\n",
        "            .inverse_transform(out.cpu().detach().numpy())\n",
        "            .reshape(-1)\n",
        "        )\n",
        "\n",
        "        targets.append(\n",
        "            label_scalers[file].inverse_transform(labels.numpy()).reshape(-1)\n",
        "        )\n",
        "\n",
        "    # Merge all files\n",
        "    concatenated_outputs = np.concatenate(outputs)\n",
        "    concatenated_targets = np.concatenate(targets)\n",
        "\n",
        "    print(f\"Evaluation Time: {time.process_time()-start_time}\")\n",
        "    print(f\"sMAPE: {round(sMAPE(concatenated_outputs, concatenated_targets), 3)}%\")\n",
        "\n",
        "    # list of of targets/outputs for each state\n",
        "    return outputs, targets, sMAPE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nUaRM_XcXg_"
      },
      "source": [
        "## Evaluate performance of GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQl9DSfTcXg_"
      },
      "outputs": [],
      "source": [
        "gru_outputs, targets, gru_sMAPE = evaluate(gru_model, test_x, test_y, label_scalers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kS1-eDwFcXhA"
      },
      "source": [
        "## Evaluate performance of LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sk3umSy3cXhA"
      },
      "outputs": [],
      "source": [
        "lstm_outputs, targets, lstm_sMAPE = evaluate(lstm_model, test_x, test_y, label_scalers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LokhnViAcXhA"
      },
      "source": [
        "While the GRU model may have made smaller errors and edged the LSTM model slightly in terms of performance accuracy, the difference is insignificant and thus inconclusive. There have been many other tests conducted by others comparing both these models but there has largely been no clear winner as to which is the better architecture overall."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jf6T9LStcXhA"
      },
      "outputs": [],
      "source": [
        "len(\n",
        "    gru_outputs\n",
        ")  # list of predicted output file for each state (each element has a 1d array for that state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AGfPCNBcXhB"
      },
      "source": [
        "# Some visualizations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-5YpMBGcXhB"
      },
      "source": [
        "Lastly, let's do some visualizations on random sets of our predicted output vs the actual consumption data for some states."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqZHsNlPcXhB"
      },
      "outputs": [],
      "source": [
        "states_list = list(test_x.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x769VZXEcXhB"
      },
      "outputs": [],
      "source": [
        "states_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "III74CmRcXhC"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14, 10))\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(gru_outputs[0][-100:], \"-o\", color=\"g\", label=\"GRU Predictions\", markersize=2)\n",
        "plt.plot(\n",
        "    lstm_outputs[0][-100:], \"-o\", color=\"r\", label=\"LSTM Predictions\", markersize=2\n",
        ")\n",
        "plt.plot(targets[0][-100:], color=\"b\", label=\"Actual\")\n",
        "plt.ylabel(\"Energy Consumption (MW)\")\n",
        "plt.title(f\"Energy Consumption for {states_list[0]} state\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(gru_outputs[1][-50:], \"-o\", color=\"g\", label=\"GRU Predictions\", markersize=2)\n",
        "plt.plot(lstm_outputs[1][-50:], \"-o\", color=\"r\", label=\"LSTM Predictions\", markersize=2)\n",
        "plt.plot(targets[1][-50:], color=\"b\", label=\"Actual\")\n",
        "plt.ylabel(\"Energy Consumption (MW)\")\n",
        "plt.title(f\"Energy Consumption for {states_list[1]} state\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.plot(gru_outputs[2][:50], \"-o\", color=\"g\", label=\"GRU Predictions\", markersize=2)\n",
        "plt.plot(lstm_outputs[2][:50], \"-o\", color=\"r\", label=\"LSTM Predictions\", markersize=2)\n",
        "plt.plot(targets[2][:50], color=\"b\", label=\"Actual\")\n",
        "plt.ylabel(\"Energy Consumption (MW)\")\n",
        "plt.title(f\"Energy Consumption for {states_list[2]} state\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.plot(gru_outputs[3][:100], \"-o\", color=\"g\", label=\"GRU Predictions\", markersize=2)\n",
        "plt.plot(lstm_outputs[3][:100], \"-o\", color=\"r\", label=\"LSTM Predictions\", markersize=2)\n",
        "plt.plot(targets[3][:100], color=\"b\", label=\"Actual\")\n",
        "plt.title(f\"Energy Consumption for {states_list[3]} state\")\n",
        "plt.ylabel(\"Energy Consumption (MW)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MzTzV3tcXhC"
      },
      "source": [
        "Looks like the models are largely successful in predicting the trends of energy consumption. While they may still get some changes wrong, such as delays in predicting a drop in consumption, the predictions follow very closely to the actual line on the test set. This is due to the nature of energy consumption data and the fact that there are patterns and cyclical changes that the model can account for. Tougher time-series prediction problems such as stock price prediction or sales volume prediction may have data that is largely random or doesn’t have predictable patterns, and in such cases, the accuracy will definitely be lower."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQt10BlXcXhD"
      },
      "source": [
        "## What's next?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x70gzfqtcXhD"
      },
      "source": [
        "* Use more data.\n",
        "* Use more complex (more layers) networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yW9K4uQscXhD"
      },
      "source": [
        "## Thank you!\n",
        "\n",
        "Contact Me: [Amir Masoud Sefidian](http://www.sefidian.com/) a.m.sefidian@gmail.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLPVUnfvcXhD"
      },
      "source": [
        "Inspired from https://blog.floydhub.com/gru-with-pytorch/"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}